{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:https://keras.io/examples/vision/knowledge_distillation/\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature**2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and test dataset.\n",
    "\n",
    "x_train = 0\n",
    "y_train = 0\n",
    "\n",
    "x_test = 0\n",
    "y_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the teacher\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        Dense(32, input_shape=x_train.shape[1:]),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(15, activation='softmax'),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        Dense(32, input_shape=x_train.shape[1:]),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(128, activation = 'gelu'),\n",
    "        Dense(64, activation = 'gelu'),\n",
    "        Dense(15, activation='softmax'),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train teacher as usual\n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate teacher on data.\n",
    "teacher.fit(x_train, y_train, epochs=5)\n",
    "teacher.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assert_multinomial_distribution(input_tensor, axis):\n",
    "  \"\"\"Assert input has valid multinomial distribution along `axis`.\"\"\"\n",
    "  sum_of_multinomial_distribution = tf.reduce_sum(\n",
    "      input_tensor=input_tensor, axis=axis)\n",
    "  return [\n",
    "      tf.debugging.assert_non_negative(input_tensor),\n",
    "      tf.debugging.assert_near(\n",
    "          sum_of_multinomial_distribution,\n",
    "          tf.constant(1.0),\n",
    "          message='x and/or y is not a proper probability distribution'),\n",
    "  ]\n",
    "\n",
    "\n",
    "def _assert_valid_axis(ndims, axis):\n",
    "  \"\"\"Assert the condition `-ndims < axis <= ndims` if `axis` is not `None`.\"\"\"\n",
    "  if axis and (axis < -ndims or axis >= ndims):\n",
    "    raise ValueError('axis = %d not in [%d, %d)' % (axis, -ndims, ndims))\n",
    "\n",
    "\n",
    "def _kl_divergence_fn(true_dist, predicted_dist):\n",
    "  epsilon = 1e-7  # A small increment to add to avoid taking a log of zero.\n",
    "  return true_dist * tf.math.log(true_dist + epsilon) - true_dist * tf.math.log(\n",
    "      predicted_dist + epsilon)\n",
    "\n",
    "def jensen_shannon_divergence(\n",
    "    labels,\n",
    "    predictions,\n",
    "    axis=-1,\n",
    "    weights=1.0,\n",
    "    scope=None,\n",
    "    loss_collection=tf.compat.v1.GraphKeys.LOSSES,\n",
    "    reduction=tf.compat.v1.losses.Reduction.SUM_BY_NONZERO_WEIGHTS):\n",
    "\n",
    "  with tf.compat.v1.name_scope(scope, 'jensen_shannon_divergence',\n",
    "                               (predictions, labels, weights)) as scope:\n",
    "    labels = tf.cast(labels, tf.dtypes.float32)\n",
    "    predictions = tf.cast(predictions, tf.dtypes.float32)\n",
    "    predictions.get_shape().assert_is_compatible_with(labels.get_shape())\n",
    "    if axis is None:\n",
    "      raise ValueError('You must specify \"axis\".')\n",
    "    _assert_valid_axis(labels.get_shape().ndims, axis)\n",
    "    assert_list = _assert_multinomial_distribution(\n",
    "        labels, axis) + _assert_multinomial_distribution(predictions, axis)\n",
    "    with tf.control_dependencies(assert_list):\n",
    "      means = 0.5 * (labels + predictions)\n",
    "      divergence_tensor = 0.5 * _kl_divergence_fn(\n",
    "          labels, means) + 0.5 * _kl_divergence_fn(predictions, means)\n",
    "      divergence = tf.reduce_sum(\n",
    "          input_tensor=divergence_tensor, axis=(axis,), keepdims=True)\n",
    "      return tf.compat.v1.losses.compute_weighted_loss(\n",
    "          divergence, weights, scope, loss_collection, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train student as done usually\n",
    "student_scratch.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate student trained from scratch.\n",
    "student_scratch.fit(x_train, y_train, epochs=3)\n",
    "student_scratch.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
