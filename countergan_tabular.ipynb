{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5mmOdOHznl"
      },
      "source": [
        "# Counterfactuals benchmark on tabular datasets\n",
        "\n",
        "# Code from https://github.com/gan-counterfactuals/countergan\n",
        "\n",
        "Edits on how to inlcude differential private optimizers is added from tensorflow_privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNbfZ4CbwI_",
        "outputId": "a152bcac-e593-4f83-e9b5-ffb9412f9cf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "BASE_PATH = \"/gdrive/My Drive/counterfactuals\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7sVjpCaH3yD"
      },
      "source": [
        "## Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tW1SdKmbrbj",
        "outputId": "eb57f82e-de9d-4046-de2d-e154bdea3863"
      },
      "outputs": [],
      "source": [
        "# Install the dev version of the Alibi package\n",
        "!pip install git+https://github.com/SeldonIO/alibi.git > /dev/null\n",
        "from alibi import __version__ as alibi_version\n",
        "print(f\"Alibi version: {alibi_version}\")\n",
        "import logging\n",
        "\n",
        "alibi_logger = logging.getLogger(\"alibi\")\n",
        "alibi_logger.setLevel(\"CRITICAL\")\n",
        "\n",
        "import tensorflow as tf\n",
        "# Disabling eager execution because Alibi is not compatible with it\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "print(f\"Is TensorFlow running in eager execution mode? -----→ {tf.executing_eagerly()}\")\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aJpXl3siWZr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "os.chdir(BASE_PATH)\n",
        "\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_{date}\"\n",
        "MODELS_EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_2020-09-09\"\n",
        "if not os.path.exists(EXPERIMENT_PATH):\n",
        "    os.makedirs(EXPERIMENT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ne3HvYI8dv"
      },
      "source": [
        "## Data import and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "mAPAZpVUiks-",
        "outputId": "f7628cfa-2bd6-4c90-e027-d90f0ecf58b3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "import time\n",
        "from matplotlib import offsetbox\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "INITIAL_CLASS = 0\n",
        "DESIRED_CLASS = 1\n",
        "N_CLASSES = 2\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "tf.random.set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "# Pima indians Diabetes dataset\n",
        "# https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
        "df = pd.read_csv(f\"{BASE_PATH}/diabetes.csv\", index_col=False)\n",
        "target_column = \"Outcome\"\n",
        "immutable_features = {\"Pregnancies\", \"DiabetesPedigreeFunction\", \"Age\"}\n",
        "\n",
        "features = set(df.columns) - {target_column}\n",
        "mutable_features = features - immutable_features\n",
        "features = list(mutable_features) + list(immutable_features)\n",
        "\n",
        "x = df[features]\n",
        "y = df[target_column].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features].values, y, test_size=0.2)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "X_train = standard_scaler.fit_transform(X_train)\n",
        "X_test = standard_scaler.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "df[features].sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTcm3LgfKEtl"
      },
      "outputs": [],
      "source": [
        "def compute_reconstruction_error(x, autoencoder):\n",
        "    \"\"\"Compute the reconstruction error for a given autoencoder and data points.\"\"\"\n",
        "    preds = autoencoder.predict(x)\n",
        "    preds_flat = preds.reshape((preds.shape[0], -1))\n",
        "    x_flat = x.reshape((x.shape[0], -1))\n",
        "    return np.linalg.norm(x_flat - preds_flat, axis=1)\n",
        "\n",
        "def format_metric(metric):\n",
        "    \"\"\"Return a formatted version of a metric, with the confidence interval.\"\"\"\n",
        "    return f\"{metric.mean():.3f} ± {1.96*metric.std()/np.sqrt(len(metric)):.3f}\"\n",
        "\n",
        "def compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None):\n",
        "    \"\"\" Summarize the relevant metrics in a dictionary. \"\"\"\n",
        "    reconstruction_error = compute_reconstruction_error(counterfactuals, autoencoder)\n",
        "    delta = np.abs(samples-counterfactuals)\n",
        "    l1_distances = delta.reshape(delta.shape[0], -1).sum(axis=1)\n",
        "    prediction_gain = (\n",
        "        classifier.predict(counterfactuals)[:, DESIRED_CLASS] - \n",
        "        classifier.predict(samples)[:, DESIRED_CLASS]\n",
        "    )\n",
        "\n",
        "    metrics = dict()\n",
        "    metrics[\"reconstruction_error\"] = format_metric(reconstruction_error)\n",
        "    metrics[\"prediction_gain\"] = format_metric(prediction_gain)\n",
        "    metrics[\"sparsity\"] = format_metric(l1_distances)\n",
        "    metrics[\"latency\"] = format_metric(latencies)\n",
        "    batch_latency = batch_latency if batch_latency else sum(latencies)\n",
        "    metrics[\"latency_batch\"] = f\"{batch_latency:.3f}\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def save_experiment(method_name, samples, counterfactuals, latencies, \n",
        "                    batch_latency=None):\n",
        "    \"\"\"Create an experiment folder and save counterfactuals, latencies and metrics.\"\"\"\n",
        "    if not os.path.exists(f\"{EXPERIMENT_PATH}/{method_name}\"):\n",
        "        os.makedirs(f\"{EXPERIMENT_PATH}/{method_name}\")   \n",
        "\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/counterfactuals.npy\", counterfactuals)\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/latencies.npy\", latencies)\n",
        "\n",
        "    metrics = compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder)\n",
        "    json.dump(metrics, open(f\"{EXPERIMENT_PATH}/{method_name}/metrics.json\", \"w\"))\n",
        "    pprint(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ISlxhmjvycC"
      },
      "source": [
        "## Train classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw26uWLiw2c",
        "outputId": "b89cf614-ecd1-4638-9c6e-0c91a4d81f1b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Add, Input, ActivityRegularization\n",
        "from tensorflow.keras import Model, optimizers, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.random.set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "def create_classifier(input_shape):\n",
        "    \"\"\"Define and compile a neural network binary classifier.\"\"\" \n",
        "    model = Sequential([\n",
        "        Dense(20, activation='relu', input_shape=input_shape),\n",
        "        Dense(20, activation='relu'),\n",
        "        Dense(2, activation='softmax'),\n",
        "    ], name=\"classifier\")\n",
        "    optimizer = optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "    return model\n",
        "\n",
        "classifier = create_classifier((x.shape[1],))\n",
        "\n",
        "training = classifier.fit(X_train, y_train, batch_size=32, epochs=200, verbose=0,\n",
        "                          validation_data=(X_test, y_test),)\n",
        "print(f\"Training: loss={training.history['loss'][-1]:.4f}, \"\n",
        "      f\"accuracy={training.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Validation: loss={training.history['val_loss'][-1]:.4f}, \"\n",
        "      f\"accuracy={training.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "classifier.save(f\"{EXPERIMENT_PATH}/classifier.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gY6FCuwuZ75"
      },
      "source": [
        "## Estimate density with the reconstruction error of a (denoising) autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHvMh2YG3hRK",
        "outputId": "f44437fa-8c44-43c9-8b90-088cfeb07938"
      },
      "outputs": [],
      "source": [
        "def add_noise(x, noise_factor=1e-6):\n",
        "    x_noisy = x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape) \n",
        "    return x_noisy\n",
        "\n",
        "    \n",
        "def create_autoencoder(in_shape=(x.shape[1],)):\n",
        "    input_ = Input(shape=in_shape) \n",
        "\n",
        "    x = Dense(32, activation=\"relu\")(input_)\n",
        "    encoded = Dense(8)(x)\n",
        "    x = Dense(32, activation=\"relu\")(encoded)\n",
        "    decoded = Dense(in_shape[0], activation=\"tanh\")(x)\n",
        "\n",
        "    autoencoder = Model(input_, decoded)\n",
        "    optimizer = optimizers.Nadam()\n",
        "    autoencoder.compile(optimizer, 'mse')\n",
        "    return autoencoder\n",
        "\n",
        "autoencoder = create_autoencoder()\n",
        "training = autoencoder.fit(\n",
        "    add_noise(X_train), X_train, epochs=100, batch_size=32, shuffle=True, \n",
        "    validation_data=(X_test, X_test), verbose=0\n",
        ")\n",
        "print(f\"Training loss: {training.history['loss'][-1]:.4f}\")\n",
        "print(f\"Validation loss: {training.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "n_samples = 1000\n",
        "# Compute the reconstruction error of noise data\n",
        "samples = np.random.randn(n_samples, X_train.shape[1])\n",
        "reconstruction_error_noise = compute_reconstruction_error(samples, autoencoder)\n",
        "\n",
        "# Save and print the autoencoder metrics\n",
        "reconstruction_error = compute_reconstruction_error(X_test, autoencoder)\n",
        "autoencoder_metrics = {\n",
        "    \"reconstruction_error\": format_metric(reconstruction_error),\n",
        "    \"reconstruction_error_noise\": format_metric(reconstruction_error_noise),\n",
        "}\n",
        "json.dump(autoencoder_metrics, open(f\"{EXPERIMENT_PATH}/autoencoder_metrics.json\", \"w\"))\n",
        "pprint(autoencoder_metrics)\n",
        "\n",
        "autoencoder.save(f\"{EXPERIMENT_PATH}/autoencoder.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for private GAN\n",
        "# replace the optimizer of the generator with one of these optimizers based on your preference.\n",
        "\n",
        "import tensorflow_privacy\n",
        "\n",
        "\n",
        "privacy = True\n",
        "l2_norm_clip = 1\n",
        "noise_multiplier = 3.5\n",
        "num_microbatches = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer_sgd = tensorflow_privacy.DPKerasSGDOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "optmizer_adam = tensorflow_privacy.DPKerasAdamOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "optimizer_adagrad = tensorflow_privacy.DPKerasAdagradOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JcccJKG87kU"
      },
      "source": [
        "## Regularized Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dA5sU3Pf1k5",
        "outputId": "d973d943-cddd-43ef-dcd1-e27ac2ea412a"
      },
      "outputs": [],
      "source": [
        "from alibi.explainers import CounterFactual\n",
        "\n",
        "shape = (1,) + X_train.shape[1:]\n",
        "feature_range = (X_train.min(), X_train.max())\n",
        "\n",
        "cf = CounterFactual(classifier, shape=shape, target_proba=1.0, tol=0.5,\n",
        "                    target_class=DESIRED_CLASS, max_iter=100, lam_init=0.001,\n",
        "                    max_lam_steps=5, learning_rate_init=0.1,\n",
        "                    feature_range=feature_range)\n",
        "\n",
        "sample = X_test[0]\n",
        "\n",
        "t_initial = time.time()\n",
        "explanation = cf.explain(np.expand_dims(sample, axis=0))\n",
        "print(f\"Produced explanation in {time.time() - t_initial:.2f} seconds \")\n",
        "\n",
        "y_prob = classifier.predict(np.expand_dims(sample, axis=0))[0]\n",
        "print(f'Original prediction: {y_prob.argmax()} with probability {y_prob.max():.3f}')\n",
        "\n",
        "pred_class = explanation.cf['class']\n",
        "proba = explanation.cf['proba'][0][pred_class]\n",
        "print(f'Counterfactual prediction: {pred_class} with probability {proba:.3f}')\n",
        "\n",
        "perturbations = (explanation.cf['X'] - sample)[0]\n",
        "perturbations[-len(immutable_features):] = 0.\n",
        "print(f\"Suggested perturbations: {perturbations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "EXJ-JYqBGmxC",
        "outputId": "fec2b652-7a53-4b72-f3b9-e482e58f7888"
      },
      "outputs": [],
      "source": [
        "samples = X_test \n",
        "\n",
        "latencies = np.empty(len(samples))\n",
        "counterfactuals = np.empty_like(samples)\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    if ((i % 20) == 0) or (i == (len(samples)-1)):\n",
        "        print(f\"Iteration {i} at {datetime.now()}\")\n",
        "    t_initial = time.time()\n",
        "    try:\n",
        "        explanation = cf.explain(np.expand_dims(sample, axis=0))\n",
        "        counterfactuals[i] = explanation.cf['X']\n",
        "    except (UnboundLocalError, TypeError):  # counterfactual search failed\n",
        "        print(f\"{i}-th sampled failed\")\n",
        "        counterfactuals[i] = sample\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(\"rgd\", samples, counterfactuals, latencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWMMH-btKLOB"
      },
      "source": [
        "## Counterfactual Search Guided by Prototypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YxnNtLuKOHA"
      },
      "outputs": [],
      "source": [
        "from alibi.explainers import CounterFactualProto\n",
        "\n",
        "shape = (1,) + X_train.shape[1:]\n",
        "feature_range = (X_train.min(), X_train.max())\n",
        "\n",
        "cf_proto = CounterFactualProto(\n",
        "    classifier, shape, use_kdtree=True, theta=10., feature_range=feature_range,\n",
        "    max_iterations=200, c_steps=10\n",
        ")\n",
        "cf_proto.fit(X_train, trustscore_kwargs=None);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o4wfCfsKcge",
        "outputId": "0805439a-2a64-47b5-9e8c-97698abea977"
      },
      "outputs": [],
      "source": [
        "sample = X_test[0]\n",
        "\n",
        "\n",
        "t_initial = time.time()\n",
        "explanation = cf_proto.explain(\n",
        "    np.expand_dims(sample, axis=0), k=5, k_type='mean', target_class=[DESIRED_CLASS]\n",
        ")\n",
        "\n",
        "print(f\"Produced explanation in {time.time() - t_initial:.2f} seconds \")\n",
        "\n",
        "y_prob = classifier.predict(np.expand_dims(sample, axis=0))[0]\n",
        "print(f'Original prediction: {y_prob.argmax()} with probability {y_prob.max():.3f}')\n",
        "\n",
        "pred_class = explanation.cf['class']\n",
        "proba = explanation.cf['proba'][0][pred_class]\n",
        "print(f'Counterfactual prediction: {pred_class} with probability {proba:.3f}')\n",
        "\n",
        "perturbations = (explanation.cf['X'] - sample)[0]\n",
        "perturbations[-len(immutable_features):] = 0.\n",
        "print(f\"Suggested perturbations: {perturbations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "62zJUpDjKin9",
        "outputId": "6d825fb6-e247-47da-dfe0-3f3cd54caa82"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "samples = X_test\n",
        "\n",
        "latencies = np.empty(len(samples))\n",
        "counterfactuals = np.empty_like(samples)\n",
        "for i, sample in enumerate(samples):\n",
        "    if ((i % 20) == 0) or (i == (len(samples)-1)):\n",
        "        print(f\"{i+1}-th iteration at {datetime.now()}\")\n",
        "    t_initial = time.time()\n",
        "    try:\n",
        "        explanation = cf_proto.explain(np.expand_dims(sample, axis=0), k=20, \n",
        "                                       k_type='mean', target_class=[DESIRED_CLASS])\n",
        "        counterfactuals[i] = explanation.cf['X']\n",
        "    except (UnboundLocalError, TypeError) as e:  # counterfactual search failed\n",
        "        if verbose:\n",
        "            print(f\"{i}-th sampled failed\")\n",
        "        counterfactuals[i] = sample\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(\"csgp\", samples, counterfactuals, latencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwoisov75MsD"
      },
      "source": [
        "## GAN-based counterfactual search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi9faGZ42qRR"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(x, generator):\n",
        "    \"\"\"Use the input generator to generate samples.\"\"\"\n",
        "    return generator.predict(x)\n",
        "\n",
        "def data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Generate batches until exhaustion of the input data.\"\"\"\n",
        "    n_train = x.shape[0]\n",
        "    if y is not None:\n",
        "        assert n_train == len(y)\n",
        "    n_complete_batches, leftover = divmod(n_train, batch_size)\n",
        "    n_batches = n_complete_batches + bool(leftover)\n",
        "\n",
        "    perm = np.random.permutation(n_train)\n",
        "    for i in range(n_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        if y is not None:\n",
        "            output = (x[batch_idx], y[batch_idx])\n",
        "        else:\n",
        "            output = x[batch_idx]\n",
        "        yield output\n",
        "\n",
        "\n",
        "def infinite_data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Infinite batch generator.\"\"\"\n",
        "    batches = data_stream(x, y, batch_size=batch_size)\n",
        "    while True:\n",
        "        try:\n",
        "            yield next(batches)\n",
        "        except StopIteration:\n",
        "            batches = data_stream(x, y, batch_size=batch_size)\n",
        "            yield next(batches)\n",
        "\n",
        "def create_generator(in_shape=(x_train.shape[1],), residuals=True):\n",
        "    \"\"\"Define and compile the residual generator of the CounteRGAN.\"\"\"\n",
        "    generator_input = Input(shape=in_shape, name='generator_input')\n",
        "    generator = Dense(64, activation='relu')(generator_input)\n",
        "    generator = Dense(32, activation='relu')(generator)\n",
        "    generator = Dense(64, activation='relu')(generator)\n",
        "    generator = Dense(in_shape[0], activation='tanh')(generator)\n",
        "    generator_output = ActivityRegularization(l1=0., l2=1e-6)(generator)\n",
        "    \n",
        "    if residuals:\n",
        "        generator_output = Add(name=\"output\")([generator_input, generator_output])\n",
        "\n",
        "    return Model(inputs=generator_input, outputs=generator_output)\n",
        "\n",
        "\n",
        "def create_discriminator(in_shape=(X_train.shape[1],)):\n",
        "    \"\"\" Define a neural network binary classifier to classify real and generated \n",
        "    examples.\"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=in_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ], name=\"discriminator\")\n",
        "    optimizer = optimizers.Adam(lr=0.0005, beta_1=0.5, decay=1e-8)\n",
        "    model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def define_countergan(generator, discriminator, classifier, \n",
        "                      input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator, discriminator, and fixed classifier into the CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(\n",
        "        inputs=countergan_input, \n",
        "        outputs=[discriminator(x_generated), classifier(x_generated)]\n",
        "    )\n",
        "        \n",
        "    optimizer = optimizers.RMSprop(lr=2e-4, decay=1e-8) # to be changed for privacy based on the optimizer chosen (check cell above)\n",
        "    countergan.compile(optimizer, [\"binary_crossentropy\", \"categorical_crossentropy\"])\n",
        "    return countergan\n",
        "\n",
        "\n",
        "def define_weighted_countergan(generator, discriminator, \n",
        "                               input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator and a discriminator for the weighted version of the \n",
        "    CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(inputs=countergan_input, outputs=discriminator(x_generated)) # to be changed for privacy based on the optimizer chosen (check cell above)\n",
        "    optimizer = optimizers.RMSprop(lr=5e-4, decay=1e-8)\n",
        "    countergan.compile(optimizer, \"binary_crossentropy\")  \n",
        "    return countergan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufnw6FKdPpa2"
      },
      "outputs": [],
      "source": [
        "def train_countergan(n_discriminator_steps, n_generator_steps, n_training_iterations,\n",
        "                     classifier, discriminator, generator, batches, \n",
        "                     weighted_version=False):\n",
        "    \"\"\" Main function: train the CounteRGAN\"\"\"\n",
        "    def check_divergence(x_generated):\n",
        "        return np.all(np.isnan(x_generated))\n",
        "\n",
        "    def print_training_information(generator, classifier, X_test, iteration):\n",
        "        X_gen = generator.predict(X_test)\n",
        "        clf_pred_test = classifier.predict(X_test)\n",
        "        clf_pred = classifier.predict(X_gen)\n",
        "\n",
        "        delta_clf_pred = (clf_pred - clf_pred_test)[:, DESIRED_CLASS]\n",
        "        y_target = to_categorical([DESIRED_CLASS] * len(clf_pred), \n",
        "                                  num_classes=N_CLASSES)\n",
        "        print('='*88)\n",
        "        print(f\"Training iteration {iteration} at {datetime.now()}\")\n",
        "        \n",
        "        \n",
        "        reconstruction_error = np.mean(compute_reconstruction_error(X_gen, autoencoder))\n",
        "        print(f\"Autoencoder reconstruction error (infinity to 0): {reconstruction_error:.3f}\")\n",
        "        print(f\"Counterfactual prediction gain (0 to 1): {delta_clf_pred.mean():.3f}\")\n",
        "        print(f\"Sparsity (L1, infinity to 0): {np.mean(np.abs(X_gen-X_test)):.3f}\")\n",
        "\n",
        "    if weighted_version:\n",
        "        countergan = define_weighted_countergan(generator, discriminator)\n",
        "    else:\n",
        "        countergan = define_countergan(generator, discriminator, classifier)\n",
        "\n",
        "    for iteration in range(n_training_iterations):\n",
        "        if iteration > 0:\n",
        "            x_generated = generator.predict(x_fake_input)\n",
        "            if check_divergence(x_generated):\n",
        "                print(\"Training diverged with the following loss functions:\")\n",
        "                print(discrim_loss_1, discrim_accuracy, gan_loss, \n",
        "                    discrim_loss, discrim_loss_2, clf_loss)\n",
        "                break\n",
        "\n",
        "        # Periodically print and plot training information \n",
        "        if (iteration % 1000 == 0) or (iteration == n_training_iterations - 1):\n",
        "            print_training_information(generator, classifier, X_test, iteration)\n",
        "\n",
        "        # Train the discriminator\n",
        "        discriminator.trainable = True\n",
        "        for _ in range(n_discriminator_steps):\n",
        "            x_fake_input, _ = next(batches)\n",
        "            x_fake = generate_fake_samples(x_fake_input, generator)\n",
        "            x_real = x_fake_input\n",
        "\n",
        "            x_batch = np.concatenate([x_real, x_fake])\n",
        "            y_batch = np.concatenate([np.ones(len(x_real)), np.zeros(len(x_fake))])\n",
        "            \n",
        "            # Shuffle real and fake examples\n",
        "            p = np.random.permutation(len(y_batch))\n",
        "            x_batch, y_batch = x_batch[p], y_batch[p]\n",
        "\n",
        "            if weighted_version:\n",
        "                classifier_scores = classifier.predict(x_batch)[:, DESIRED_CLASS]\n",
        "                \n",
        "                # The following update to the classifier scores is needed to have the \n",
        "                # same order of magnitude between real and generated samples losses\n",
        "                real_samples = np.where(y_batch == 1.)\n",
        "                average_score_real_samples = np.mean(classifier_scores[real_samples])\n",
        "                classifier_scores[real_samples] /= average_score_real_samples\n",
        "                \n",
        "                fake_samples = np.where(y_batch == 0.)\n",
        "                classifier_scores[fake_samples] = 1.\n",
        "\n",
        "                discriminator.train_on_batch(\n",
        "                    x_batch, y_batch, sample_weight=classifier_scores\n",
        "                )\n",
        "            else:\n",
        "                discriminator.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "        # Train the generator \n",
        "        discriminator.trainable = False\n",
        "        for _ in range(n_generator_steps):\n",
        "            x_fake_input, _ = next(batches)\n",
        "            y_fake = np.ones(len(x_fake_input))\n",
        "            if weighted_version:\n",
        "                countergan.train_on_batch(x_fake_input, y_fake)\n",
        "            else:\n",
        "                y_target = to_categorical([DESIRED_CLASS] * len(x_fake_input), \n",
        "                                          num_classes=N_CLASSES)\n",
        "                countergan.train_on_batch(x_fake_input, [y_fake, y_target])\n",
        "    return countergan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpaZptCP2k_U"
      },
      "source": [
        "## Counterfactual search with a regular GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "L4PL01njRnV9",
        "outputId": "dbaea06d-6f95-4bf7-c02e-15daaeed5164"
      },
      "outputs": [],
      "source": [
        "discriminator = create_discriminator()\n",
        "generator = create_generator(residuals=False)\n",
        "batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "\n",
        "method_name = \"regular_gan\"\n",
        "countergan = train_countergan(2, 4, 2000, classifier, discriminator, generator, batches)\n",
        "\n",
        "t_initial = time.time()\n",
        "counterfactuals = generator.predict(X_test)\n",
        "batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "latencies = np.zeros(len(X_test))\n",
        "for i, x in enumerate(X_test):\n",
        "    t_initial = time.time()\n",
        "    _ = generator.predict(np.expand_dims(x, axis=0))\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IvGXenSWL-"
      },
      "source": [
        "## CounteRGAN: first formulation for differentiable classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "mNQmxFS5SXIH",
        "outputId": "8ab0b191-abd2-44f3-ecd6-f980df21bd1c"
      },
      "outputs": [],
      "source": [
        "discriminator = create_discriminator()\n",
        "generator = create_generator(residuals=True)\n",
        "batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "\n",
        "method_name = \"countergan\"\n",
        "countergan = train_countergan(2, 4, 2000, classifier, discriminator, generator, batches)\n",
        "\n",
        "t_initial = time.time()\n",
        "counterfactuals = generator.predict(X_test)\n",
        "batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "latencies = np.zeros(len(X_test))\n",
        "for i, x in enumerate(X_test):\n",
        "    t_initial = time.time()\n",
        "    _ = generator.predict(np.expand_dims(x, axis=0))\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pf5VJKqTBfd"
      },
      "source": [
        "## CounteRGAN: second formulation for any classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "LKjswCOCTC-4",
        "outputId": "036b03e9-76ac-4f42-ac37-e85f0bbf28a6"
      },
      "outputs": [],
      "source": [
        "discriminator = create_discriminator()\n",
        "generator = create_generator(residuals=True)\n",
        "batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "\n",
        "method_name = \"countergan-wt\"\n",
        "countergan = train_countergan(2, 3, 2000, classifier, discriminator, generator, \n",
        "                              batches, weighted_version=True)\n",
        "\n",
        "t_initial = time.time()\n",
        "counterfactuals = generator.predict(X_test)\n",
        "batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "latencies = np.zeros(len(X_test))\n",
        "for i, x in enumerate(X_test):\n",
        "    t_initial = time.time()\n",
        "    _ = countergan.predict(np.expand_dims(x, axis=0))\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T-D0Jj0LVJM"
      },
      "source": [
        "## Generate the benchmark table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "5w-2mCNALc8O",
        "outputId": "5adf7fa7-ef41-43d5-df19-c6f1eabfabf2"
      },
      "outputs": [],
      "source": [
        "METHODS = [\"rgd\", \"csgp\", \"regular_gan\", \"countergan\", \"countergan-wt\"]\n",
        "METRIC_NAMES = [\n",
        "    \"prediction_gain\", \"reconstruction_error\", \"sparsity\", \"latency\", \"latency_batch\"\n",
        "]\n",
        "\n",
        "metrics = dict()\n",
        "for method in METHODS:\n",
        "    method_metrics = json.load(open(f\"{EXPERIMENT_PATH}/{method}/metrics.json\", \"r\"))\n",
        "    method_metrics = {k: v for k, v in method_metrics.items() if k in METRIC_NAMES}\n",
        "    metrics[method] = method_metrics\n",
        "\n",
        "metrics = pd.DataFrame(metrics)\n",
        "metrics.columns =  [\"RGD\",  \"CSGP\", \"GAN\", \"CounterGAN\", \"CounterRGAN-wt\"] \n",
        "\n",
        "metrics.index = [\n",
        "    \"↓ Realism\",\n",
        "    \"↑ Prediction gain\",\n",
        "    \"↓ Sparsity\",\n",
        "    \"↓ Latency (ms)\",\n",
        "    \"↓ Batch latency (ms)\",\n",
        "]\n",
        "\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKAfN55k9zjd"
      },
      "source": [
        "## Individual examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "s69m-GMnXxNf",
        "outputId": "facf23e6-0098-4842-b783-d5cc91218964"
      },
      "outputs": [],
      "source": [
        "negative_idx = np.where(classifier.predict(x_test)[:, 1] < 0.5)[0]\n",
        "x_negative = X_test[negative_idx]\n",
        "original_features = standard_scaler.inverse_transform(x_negative)\n",
        "negative_df = pd.DataFrame(original_features, columns=features)\n",
        "negative_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Ud0Dwe0GZXFT",
        "outputId": "5c247d8b-e9a7-42e2-95d0-35492781d306"
      },
      "outputs": [],
      "source": [
        "counterfactuals = standard_scaler.inverse_transform(\n",
        "    generator.predict(x_test[negative_idx])\n",
        ")\n",
        "residuals = (counterfactuals - \n",
        "             standard_scaler.inverse_transform(x_test[negative_idx]))\n",
        "residuals_df = pd.DataFrame(residuals, columns=features)\n",
        "residuals_df[list(immutable_features)] = 0.\n",
        "residuals_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "qW9zy0bTYM5h",
        "outputId": "e74d92aa-3aa2-4d08-80d6-7a836c7b86c5"
      },
      "outputs": [],
      "source": [
        "sample_idx = 20\n",
        "sample = np.expand_dims(X_test[sample_idx], axis=0)\n",
        "\n",
        "def compute_residuals(sample, counterfactual):\n",
        "    counterfactual = standard_scaler.inverse_transform(counterfactual)\n",
        "    residuals = (counterfactual - standard_scaler.inverse_transform(sample))[0]\n",
        "    residuals[-len(immutable_features):] = 0\n",
        "    return residuals\n",
        "\n",
        "method_outputs = dict()\n",
        "\n",
        "d = negative_df.iloc[sample_idx].to_dict()\n",
        "d[\"Classifier Prediction\"] = classifier.predict(sample)[0][1]\n",
        "method_outputs[\"Initial values\"] = d\n",
        "\n",
        "\n",
        "explanation = cf.explain(sample)\n",
        "counterfactual = explanation.cf['X']\n",
        "scaled_counterfactual = compute_residuals(sample, counterfactual)\n",
        "d = {k: v for k, v in zip(features, list(scaled_counterfactual))}\n",
        "d[\"Classifier Prediction\"] = classifier.predict(counterfactual)[0][1]\n",
        "method_outputs[\"RGD\"] = d\n",
        "\n",
        "explanation = cf_proto.explain(sample, k=5, k_type='mean', target_class=[DESIRED_CLASS])\n",
        "counterfactual = explanation.cf['X']\n",
        "scaled_counterfactual = compute_residuals(sample, counterfactual)\n",
        "d = {k: v for k, v in zip(features, list(scaled_counterfactual))}\n",
        "d[\"Classifier Prediction\"] = classifier.predict(counterfactual)[0][1]\n",
        "method_outputs[\"CSGP\"] = d\n",
        "\n",
        "for method in [\"regular_gan\", \"countergan\", \"countergan-wt\"]:\n",
        "    generator = load_model(f\"{EXPERIMENT_PATH}/{method}/generator.h5\")\n",
        "    counterfactual = generator.predict(sample)\n",
        "    scaled_counterfactual = compute_residuals(sample, counterfactual)\n",
        "    d = {k: v for k, v in zip(features, list(scaled_counterfactual))}\n",
        "    d[\"Classifier Prediction\"] = classifier.predict(counterfactual)[0][1]\n",
        "    method_outputs[method] = d\n",
        "\n",
        "df = pd.DataFrame(method_outputs)\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "kdd_counterfactuals_diabetes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
